{
  "file_path": ".specstory/history/2025-09-13_23-26Z-change-code-to-parse-multiple-files.md",
  "main_timestamp": "2025-09-13 23:26Z",
  "title": "Change code to parse multiple files",
  "conversations": [
    {
      "type": "user",
      "timestamp": "2025-09-13 12:50Z",
      "content": "change this code that I can parse all the files instead of specifiy one"
    },
    {
      "type": "assistant",
      "content": "---\n\nI'll modify the code to parse all markdown files in a directory instead of just one file. Let me update the script to handle multiple files:\n\n```python:clean_data/parse_specstory.py\n#!/usr/bin/env python3\n\"\"\"\nParser for .specstory markdown files to extract conversations and todo lists.\n\"\"\"\n\nimport re\nimport json\nimport os\nfrom typing import Dict, List, Any, Optional\nfrom datetime import datetime\nimport sys\nfrom pathlib import Path\nimport glob\n\nclass SpecStoryParser:\n    def __init__(self, file_path: str):\n        self.file_path = file_path\n        self.content = self._read_file()\n\n    def _read_file(self) -> str:\n        \"\"\"Read the markdown file content.\"\"\"\n        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n            return f.read()\n\n    def parse(self) -> Dict[str, Any]:\n        \"\"\"Parse the entire document and return structured data.\"\"\"\n        data = {\n            \"file_path\": self.file_path,\n            \"main_timestamp\": self._extract_main_timestamp(),\n            \"title\": self._extract_title(),\n            \"conversations\": self._extract_conversations(),\n            \"all_todo_lists\": [],\n        }\n\n        # Collect all todo lists from conversations\n        for conv in data[\"conversations\"]:\n            if conv[\"type\"] == \"assistant\" and \"todo_lists\" in conv:\n                data[\"all_todo_lists\"].extend(conv[\"todo_lists\"])\n\n        return data\n\n    # ... existing methods remain unchanged ...\n\n    def _extract_todo_lists(self, content: str) -> List[Dict[str, Any]]:\n        \"\"\"Extract all todo lists from content.\"\"\"\n        todo_lists = []\n\n        # Find all todo list sections\n        todo_pattern = r\"##\\s+Todo List\\s*\\n(.*?)(?=\\n---|\\n##|\\Z)\"\n        todo_matches = re.finditer(todo_pattern, content, re.DOTALL)\n\n        for match in todo_matches:\n            todo_content = match.group(1).strip()\n            items = []\n\n            # Parse individual todo items\n            item_pattern = r\"-\\s*\\[([ x-])\\]\\s*(.+?)(?=\\n-|\\Z)\"\n            item_matches = re.finditer(item_pattern, todo_content, re.DOTALL)\n\n            for item_match in item_matches:\n                status_char = item_match.group(1)\n                task = item_match.group(2).strip()\n\n                # Map status characters to status strings\n                status_map = {\"x\": \"completed\", \"-\": \"in_progress\", \" \": \"pending\"}\n                status = status_map.get(status_char, \"pending\")\n\n                items.append({\"task\": task, \"status\": status})\n\n            if items:\n                todo_lists.append(\n                    {\n                        \"items\": items,\n                        \"total_items\": len(items),\n                        \"completed_items\": sum(\n                            1 for item in items if item[\"status\"] == \"completed\"\n                        ),\n                        \"in_progress_items\": sum(\n                            1 for item in items if item[\"status\"] == \"in_progress\"\n                        ),\n                        \"pending_items\": sum(\n                            1 for item in items if item[\"status\"] == \"pending\"\n                        ),\n                    }\n                )\n\n        return todo_lists\n\ndef process_file(file_path: str, output_dir: str) -> Dict[str, Any]:\n    \"\"\"Process a single file and save the results.\"\"\"\n    try:\n        parser = SpecStoryParser(file_path)\n        data = parser.parse()\n        \n        # Generate output file path\n        base_name = os.path.basename(file_path).replace(\".md\", \"_parsed.json\")\n        output_file = os.path.join(output_dir, base_name)\n        \n        # Save to JSON file\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f, indent=2, ensure_ascii=False)\n        \n        return {\n            \"status\": \"success\",\n            \"file\": file_path,\n            \"output\": output_file,\n            \"conversations\": len(data[\"conversations\"]),\n            \"todo_lists\": len(data[\"all_todo_lists\"])\n        }\n    except Exception as e:\n        return {\n            \"status\": \"error\",\n            \"file\": file_path,\n            \"error\": str(e)\n        }\n\ndef find_markdown_files(path: str, pattern: str = \"*.md\") -> List[str]:\n    \"\"\"Find all markdown files in the given path.\"\"\"\n    path_obj = Path(path)\n    \n    if path_obj.is_file():\n        # If it's a single file, return it as a list\n        return [str(path_obj)] if path_obj.suffix == \".md\" else []\n    elif path_obj.is_dir():\n        # If it's a directory, find all matching files recursively\n        return sorted(glob.glob(str(path_obj / \"**\" / pattern), recursive=True))\n    else:\n        # Try as a glob pattern\n        return sorted(glob.glob(path, recursive=True))\n\ndef main():\n    \"\"\"Main function to run the parser.\"\"\"\n    if len(sys.argv) < 2:\n        print(\"Usage: python parse_specstory.py <path_to_directory_or_file> [pattern]\")\n        print(\"Examples:\")\n        print(\"  python parse_specstory.py .specstory/history/\")\n        print(\"  python parse_specstory.py .specstory/history/ '*.md'\")\n        print(\"  python parse_specstory.py path/to/single/file.md\")\n        sys.exit(1)\n\n    input_path = sys.argv[1]\n    pattern = sys.argv[2] if len(sys.argv) > 2 else \"*.md\"\n    \n    # Find all markdown files\n    files = find_markdown_files(input_path, pattern)\n    \n    if not files:\n        print(f\"❌ No markdown files found in: {input_path}\")\n        sys.exit(1)\n    \n    print(f\"\\n=== SpecStory Parser ===\")\n    print(f\"Found {len(files)} file(s) to process\")\n    \n    # Determine output directory\n    if Path(input_path).is_file():\n        # For single file, create cleaned_data next to the file\n        output_dir = os.path.join(os.path.dirname(input_path), \"cleaned_data\")\n    else:\n        # For directory, create cleaned_data in current working directory\n        output_dir = os.path.join(os.getcwd(), \"cleaned_data\")\n    \n    os.makedirs(output_dir, exist_ok=True)\n    print(f\"Output directory: {output_dir}\\n\")\n    \n    # Process all files\n    results = []\n    for i, file_path in enumerate(files, 1):\n        print(f\"[{i}/{len(files)}] Processing: {file_path}\")\n        result = process_file(file_path, output_dir)\n        results.append(result)\n        \n        if result[\"status\"] == \"success\":\n            print(f\"  ✅ Success - {result['conversations']} conversations, {result['todo_lists']} todo lists\")\n            print(f\"  📁 Saved to: {result['output']}\")\n        else:\n            print(f\"  ❌ Error: {result['error']}\")\n    \n    # Summary\n    print(f\"\\n=== Summary ===\")\n    successful = sum(1 for r in results if r[\"status\"] == \"success\")\n    failed = sum(1 for r in results if r[\"status\"] == \"error\")\n    \n    print(f\"Total files processed: {len(files)}\")\n    print(f\"  ✅ Successful: {successful}\")\n    print(f\"  ❌ Failed: {failed}\")\n    \n    if failed > 0:\n        print(f\"\\nFailed files:\")\n        for r in results:\n            if r[\"status\"] == \"error\":\n                print(f\"  - {r['file']}: {r['error']}\")\n    \n    # Save summary report\n    summary_file = os.path.join(output_dir, \"parsing_summary.json\")\n    with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump({\n            \"timestamp\": datetime.now().isoformat(),\n            \"input_path\": input_path,\n            \"pattern\": pattern,\n            \"total_files\": len(files),\n            \"successful\": successful,\n            \"failed\": failed,\n            \"results\": results\n        }, f, indent=2)\n    \n    print(f\"\\n📊 Summary report saved to: {summary_file}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nThe key changes I made:\n\n1. **Added file discovery functionality** - The `find_markdown_files()` function can handle:\n   - Single file paths\n   - Directory paths (searches recursively for all `.md` files)\n   - Glob patterns for more flexibility\n\n2. **Separated file processing** - Created `process_file()` function to handle individual file parsing and saving, making it reusable for multiple files\n\n3. **Enhanced command-line interface**:\n   - Can now accept a directory path or file path\n   - Optional second argument for file pattern (defaults to `*.md`)\n   - Better usage instructions\n\n4. **Progress tracking** - Shows progress as it processes multiple files with success/error status\n\n5. **Summary report** - Creates a `parsing_summary.json` file with details about all processed files\n\n6. **Flexible output directory**:\n   - For single files: creates `cleaned_data` next to the file\n   - For directories: creates `cleaned_data` in current working directory\n\nNow you can use it in several ways:\n\n---\n\n---\n\n```bash\n# Parse all .md files in a directory recursively\npython clean_data/parse_specstory.py .specstory/history/\n\n# Parse with a specific pattern\npython clean_data/parse_specstory.py . \"**/*.specstory.md\"\n\n# Still works with single files\npython clean_data/parse_specstory.py path/to/single/file.md\n```\n\nThe script will now:\n- Find all markdown files in the specified directory (recursively)\n- Process each file individually\n- Show progress and results for each file\n- Create a summary report with statistics\n- Handle errors gracefully without stopping the entire batch"
    }
  ],
  "all_todo_lists": []
}